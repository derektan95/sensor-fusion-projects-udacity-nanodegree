# Tutorial for Sensor Fusion Nanodegree - Udacity Course

Date Attempted: 5th July 2020

**Summary**<br/>
This is a course that summarizes the essential principles of LiDAR, Camera, Radar and Sensor Fusion. It contains my own implementaion of functions necessary to pass the project specifications of the udacity project.

## LiDAR Segment - Point Cloud Library<br/>
The main principles taught in this segment are: 
1) Plane Segmentation
2) Euclidean Clustering using kd-tree
3) Filtering techniques
4) Reading and streaming PCDs

<img src="media/obstacle_detect_point_cloud_streaming.gif" width="900" height="400" />

## Camera Segment - OpenCV<br/>
The main principles taught in this segment are: 
1) Camera Technology & Optics
2) Autonomous Vehicles & Computer Vision 
3) Engineering a Collision Detection System (Time To Collision)
4) Tracking Image Features (Detectors, Descriptors, Matchers, Selectors)
5) Introduction to Object Detection Frameworks - YOLO
6) Sensor Fusion - Camera + LiDAR

<img src="media/time_to_collision_with_keypt_match_gif.gif" width="1000" height="400" />

## Radar Segment - Matlab<br/>
The main principles taught in this segment are: 
1) Radar Principles
2) Using Fast-Fourier Transform to obtain Range (Distance) and Doppler (Velocity) information
3) Removing clutter (noise), using Cell Averaging Constant Fast Alarm Rate (CA-CFAR)
4) Clustering and Tracking of Radar points
5) Simulation on Matlab

<img src="media/range_doppler_map_obstacle_radar.jpg" width="900" height="400" />

## Kalman Filter Segment<br/>
The main principles taught in this segment are: 
1) Normal Kalman Filter (1D)
2) Extended Kalman Filter (2D)
3) Unscented Kalman Filter (2D)

<img src="media/unscented_kalman_filter_simulation.gif" width="1400" height="400" />
